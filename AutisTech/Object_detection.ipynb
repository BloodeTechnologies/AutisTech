{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import cv2\n",
    "torch.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alast\\OneDrive\\Documents\\projects\\Sensory_Remover\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alast\\OneDrive\\Documents\\projects\\Sensory_Remover\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSD300_VGG16_Weights.COCO_V1`. You can also use `weights=SSD300_VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SSD(\n",
       "  (backbone): SSDFeatureExtractorVGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "    )\n",
       "    (extra): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "      (3-4): 2 x Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (anchor_generator): DefaultBoxGenerator(aspect_ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]], clip=True, scales=[0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05], steps=[8, 16, 32, 64, 100, 300])\n",
       "  (head): SSDHead(\n",
       "    (classification_head): SSDClassificationHead(\n",
       "      (module_list): ModuleList(\n",
       "        (0): Conv2d(512, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(1024, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(512, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4-5): 2 x Conv2d(256, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (regression_head): SSDRegressionHead(\n",
       "      (module_list): ModuleList(\n",
       "        (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4-5): 2 x Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.48235, 0.45882, 0.40784], std=[0.00392156862745098, 0.00392156862745098, 0.00392156862745098])\n",
       "      Resize(min_size=(300,), max_size=300, mode='bilinear')\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'street sign', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'hat', 'backpack', 'umbrella', 'shoe', 'eye glasses', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'plate', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'mirror', 'dining table', 'window', 'desk', 'toilet', 'door', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'blender', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'hair brush']\n"
     ]
    }
   ],
   "source": [
    "classnames = []\n",
    "with open ('classes.txt', 'r') as f:\n",
    "    classnames = f.read().splitlines()\n",
    "    print(classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    return image\n",
    "\n",
    "def transform_image(image):\n",
    "    img_transform = T.ToTensor()\n",
    "    image_tensor = img_transform(image)\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import entity\n",
    "\n",
    "def detect_objects(model, image_tensor, entities, confidence_threshold=0.8):\n",
    "    \n",
    "    ## The entity system is going to determine if there are any known entities that were recently found\n",
    "    ## within the frame. It'll use this to remove the entity scan areas from image_tensor, turning them into \n",
    "    ## zeros. This should improve the speed for the search for new entities.\n",
    "    start_time = time.time()\n",
    "    print(start_time)\n",
    "    entities = {}\n",
    "    if entities == {}:\n",
    "        with torch.no_grad():\n",
    "            y_pred = model([image_tensor])\n",
    "        \n",
    "        bbox, scores, labels = y_pred[0]['boxes'], y_pred[0]['scores'], y_pred[0]['labels']\n",
    "        indices = torch.nonzero(scores > confidence_threshold).squeeze(1)\n",
    "        \n",
    "        filtered_bbox = bbox[indices]\n",
    "        filtered_scores = scores[indices]\n",
    "        filtered_labels = labels[indices]\n",
    "        \n",
    "        if len(filtered_bbox) == len(filtered_labels):\n",
    "            new_entities = []\n",
    "            for l, i in enumerate(filtered_labels):\n",
    "                x, y, w, h = bbox[i].numpy().astype('int')\n",
    "                class_label = classnames[l]\n",
    "                if class_label not in entities.keys():\n",
    "                    entities[class_label] = []\n",
    "                entity = {\"bbox\":(x, y,w,h), \"label\":classnames[l]}\n",
    "                print(entity)\n",
    "                entities[class_label].append(entity)\n",
    "        end_time = time.time()\n",
    "        print(f\"time taken: {end_time - start_time}\")\n",
    "        return filtered_bbox, filtered_scores, filtered_labels\n",
    "    else:\n",
    "        print(\"There are known entities\")\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"time taken: {end_time - start_time}\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is an issue with the system currently. It's not drawing the label.\n",
    "def draw_boxes_and_labels(image, bbox, labels, class_names):\n",
    "    img_copy = image.copy()\n",
    "    \n",
    "    for i in range(len(bbox)):\n",
    "        x, y, w, h = bbox[i].numpy().astype('int')\n",
    "        cv2.rectangle(img_copy, (x, y), (w, h), (0, 0, 255), 2)\n",
    "        \n",
    "        class_index = labels[i].numpy().astype('int')\n",
    "        class_detected = class_names[class_index]\n",
    "        try:\n",
    "            cv2.putText(img_copy, class_detected, (x, y-10), cv2.FONT_HERSHEY_COMPLEX, .5, (0, 180, 0), 1, cv2.LINE_AA)\n",
    "        except:\n",
    "            continue        \n",
    "    return img_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 480, 640])\n",
      "1706859711.6011868\n",
      "{'bbox': (84, 351, 103, 400), 'label': 'person'}\n",
      "{'bbox': (84, 351, 103, 400), 'label': 'bicycle'}\n",
      "time taken: 0.4830169677734375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_path = '../Visuals/test2.jpg'\n",
    "img = load_image(image_path)\n",
    "img_tensor = transform_image(img)\n",
    "print(img_tensor.shape)\n",
    "bbox, scores, labels = detect_objects(model, img_tensor, entities=[])\n",
    "\n",
    "result_img = draw_boxes_and_labels(img, bbox, labels, classnames)\n",
    "\n",
    "\n",
    "cv2.imshow(\"The Image\", result_img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_similar_entity(entity_image, scan_image):\n",
    "    \n",
    "    entity_image.resize_(3, 220, 220)\n",
    "    scan_image.resize_(3, 220, 220)\n",
    "    \n",
    "    cosi = torch.nn.CosineSimilarity(dim=0)\n",
    "    sim = cosi(entity_image, scan_image)\n",
    "    sim = torch.mean(sim)\n",
    "    if sim > .7:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def capture_and_display_current_frame_rate(image, start_time):\n",
    "    fr = round(1.0/(time.time() - start_time))\n",
    "    image = cv2.putText(image, f\"Frame Rate: {str(fr) if fr < 120 else 120}\", (0, 15), cv2.FONT_HERSHEY_COMPLEX, .5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    return round(1.0 / (time.time() - start_time)), image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_image_detection_1(frame):\n",
    "    ### This is our general image detection. It uses it's model to determine if any of those classes exist in the image.\n",
    "    bbox, labels, classnames = detect_objects(model, transform_image(frame), entities, confidence_threshold=.8)\n",
    "    frame = draw_boxes_and_labels(frame, bbox, labels, classnames)\n",
    "    return frame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UI_Display():\n",
    "    def __init__(self):\n",
    "        print(\"Initializing UI\")\n",
    "        self.flags = {\"main UI settings\":{\"framerate\":True}}\n",
    "        \n",
    "    def Display_Text(self, image, text, line , size=.5, color=(), thickness = 1):\n",
    "        try:\n",
    "            cv2.putText(image, text, (0, (15*line)), cv2.FONT_HERSHEY_COMPLEX, size, color, thickness, cv2.LINE_AA)\n",
    "            return image\n",
    "        except:\n",
    "            print(\"Cannot display text. Unknown error\")\n",
    "            \n",
    "    def UI_Update(self):\n",
    "        for f in self.flags.keys():\n",
    "            print(\"f=\", f, type(f))\n",
    "            if type(self.flags[f]) == dict:\n",
    "                for sub in self.flags[f].keys():\n",
    "                    print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing UI\n",
      "1706859730.2142894\n",
      "time taken: 0.39067578315734863\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n",
      "1706859730.6312456\n",
      "time taken: 0.40299129486083984\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n",
      "1706859731.0418806\n",
      "time taken: 0.4328420162200928\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n",
      "1706859731.4827251\n",
      "time taken: 0.4147984981536865\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n",
      "1706859731.9033546\n",
      "time taken: 0.39026689529418945\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n",
      "1706859732.3048549\n",
      "time taken: 0.3917422294616699\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n",
      "1706859732.7092512\n",
      "time taken: 0.3651716709136963\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n",
      "1706859733.0833528\n",
      "time taken: 0.37475156784057617\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n",
      "1706859733.4750752\n",
      "time taken: 0.3693206310272217\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n",
      "1706859733.86471\n",
      "time taken: 0.35861706733703613\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n",
      "1706859734.2407832\n",
      "time taken: 0.3434298038482666\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n",
      "1706859734.6010263\n",
      "time taken: 0.363051176071167\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n",
      "1706859734.974925\n",
      "time taken: 0.3727991580963135\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n",
      "1706859735.362203\n",
      "time taken: 0.3513944149017334\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n",
      "1706859735.7206116\n",
      "time taken: 0.372103214263916\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n",
      "1706859736.1099167\n",
      "time taken: 0.35912275314331055\n",
      "f= main UI settings <class 'str'>\n",
      "framerate\n"
     ]
    }
   ],
   "source": [
    "### This one runs the same model through the webcam. It's slow. I do not like this. \n",
    "\n",
    "# import the opencv library \n",
    "import cv2 \n",
    "  \n",
    "# define a video capture object \n",
    "vid = cv2.VideoCapture(0) \n",
    "currentFrame = 0\n",
    "entities = []\n",
    "image_display_flags = {}\n",
    "image_display_flags[\"general_0\"] = True\n",
    "entities = {}\n",
    "display = UI_Display()\n",
    "### This is where the magic starts. We'll get the metrics to determine our frame rate, and grab the frame from the webcam. \n",
    "### All the ai and post processing happens through this central process.\n",
    "while(True): \n",
    "      \n",
    "    # Capture the video frame \n",
    "    # by frame \n",
    "    startFrame = time.time()\n",
    "    ret, frame = vid.read() \n",
    "    \n",
    "    if image_display_flags[\"general_0\"] == True:\n",
    "        frame = general_image_detection_1(frame)\n",
    "    \n",
    "        \n",
    "    # result_img = draw_boxes_and_labels(frame, bbox, labels, classnames)\n",
    "    # cv2.putText(result_img, f\"FPS: {round(1.0 / (time.time() - startFrame), 2)}\", (0, 15), cv2.FONT_HERSHEY_COMPLEX, .5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    # cv2.putText(result_img, f\"Current Frame: {currentFrame}\", (0, 30), cv2.FONT_HERSHEY_COMPLEX, .5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    fr, frame = capture_and_display_current_frame_rate(frame, startFrame)\n",
    "    display.UI_Update()\n",
    "    cv2.imshow(\"Ai Object Detection 1\", frame)\n",
    "    currentFrame += 1\n",
    "      \n",
    "    # the 'q' button is set as the \n",
    "    # quitting button you may use any \n",
    "    # desired button of your choice \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        \n",
    "        break\n",
    "  \n",
    "# After the loop release the cap object \n",
    "vid.release() \n",
    "# Destroy all the windows \n",
    "cv2.destroyAllWindows() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
